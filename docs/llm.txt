Installation
============

``ell`` and ``ell studio`` are both contained within the ``ell-ai`` python package available on PyPI. You simply need to install the package and set up your API keys.

Installing ell
--------------

1. Install using pip:

   .. code-block:: bash

      pip install -U ell-ai

   By default, this installs only the OpenAI client SDK. If you want to include the Anthropic client SDK, use the "anthropic" extra like so:

   .. code-block:: bash

      pip install -U 'ell-ai[anthropic]'

2. Verify installation:

   .. code-block:: bash

      python -c "import ell; print(ell.__version__)"

API Key Setup
-------------

OpenAI API Key
~~~~~~~~~~~~~~

1. Get API key from https://platform.openai.com/account/api-keys
2. Install the OpenAI Python package:

   .. code-block:: bash

      pip install openai

3. Set environment variable:

   - Windows:

     .. code-block:: batch

        setx OPENAI_API_KEY "your-openai-api-key"

   - macOS/Linux: 

     .. code-block:: bash

        # in your .bashrc or .zshrc
        export OPENAI_API_KEY='your-openai-api-key'

Anthropic API Key
~~~~~~~~~~~~~~~~~

1. Get API key from https://www.anthropic.com/
2. Install the Anthropic Python package:

   .. code-block:: bash

      pip install anthropic

3. Set environment variable:

   - Windows:

     .. code-block:: batch

        setx ANTHROPIC_API_KEY "your-anthropic-api-key"

   - macOS/Linux:

     .. code-block:: bash

        # in your .bashrc or .zshrc
        export ANTHROPIC_API_KEY='your-anthropic-api-key'

Troubleshooting
---------------

- Update pip: ``pip install --upgrade pip``
- Use virtual environment
- Try ``pip3`` instead of ``pip``
- Use ``sudo`` (Unix) or run as administrator (Windows) if permission errors occur

For more help, see the Troubleshooting section or file an issue on GitHub.

Next Steps
----------

Proceed to the Getting Started guide to create your first Language Model Program.



===========================================
ell: The Language Model Programming Library
===========================================


.. raw:: html
   
   <div style="display: flex; gap: 10px;">
   <a href="https://docs.ell.so/installation" target="_blank">
       <img src="https://img.shields.io/badge/get_started-blue" alt="Install">
   </a>
   <a href="https://discord.gg/vWntgU52Xb" target="_blank">
       <img src="https://dcbadge.limes.pink/api/server/vWntgU52Xb?style=flat" alt="Discord">
   </a>
   <a href="https://twitter.com/wgussml" target="_blank">
       <img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/wgussml">
   </a>
   <a href="https://jobs.ell.so" target="_blank">
       <img src="https://img.shields.io/badge/jobs-board-green" alt="Jobs Board">
   </a>
   </div>

.. title:: Introduction

``ell`` is a lightweight prompt engineering library treating prompts as functions. After years of building and using language models at OpenAI and in the startup ecosystem, ``ell`` was designed from the following principles:


Prompts are programs, not strings
----------------------------------

.. code-block:: python

    import ell

    @ell.simple(model="gpt-4o-mini")
    def hello(world: str):
        """You are a helpful assistant""" # System prompt
        name = world.capitalize()
        return f"Say hello to {name}!" # User prompt

    hello("sam altman") # just a str, "Hello Sam Altman! ..."



.. image:: _static/gif1.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%




Prompts aren't just strings; they are all the code that leads to strings being sent to a language model. In ell, we think of one particular way of using a language model as a discrete subroutine called a **language model program** (LMP).

LMPs are fully encapsulated functions that produce either a string prompt or a list of messages to be sent to various multimodal language models. This encapsulation creates a clean interface for users, who only need to be aware of the required data specified to the LMP.


Prompt engineering is an optimization process
------------------------------------------------

The process of prompt engineering involves many iterations, similar to the optimization processes in machine learning. Because LMPs are just functions, ``ell`` provides rich tooling for this process.

.. image:: _static/versions_small.webp
   :alt: ell demonstration
   :class: rounded-image .invertible-image
   :width: 100%


``ell`` provides **automatic versioning and serialization of prompts** through static and dynamic analysis and  ``gpt-4o-mini`` **autogenerated commit messages** directly to a *local store*. This process is similar to `checkpointing` in a machine learning training loop, but it doesn't require any special IDE or editor - it's all done with regular Python code.

.. code-block:: python
   :emphasize-lines: 3,3

    import ell

    ell.init(store='./logdir')  # Versions your LMPs and their calls

    # ... define your lmps

    hello("strawberry") # the source code of the LMP the call is saved to the store
   


Tools for monitoring, versioning, and visualization
-----------------------------------------------------------



.. image:: _static/ell_studio_better.webp
   :alt: ell demonstration
   :class: rounded-image 
   :width: 100%


.. code-block:: bash

   ell-studio --storage ./logdir 

Prompt engineering goes from a dark art to a science with the right tools. **Ell Studio is a local, open source tool for prompt version control, monitoring, visualization**. With Ell Studio you can empiricize your prompt optimziation process over time and catch regressions before its too late. 




Test-time compute is important
--------------------------------

.. Note on how in promtp engineering going from a demo to something that acutally works often requires test time compute (calling a lm many times) techniques like lm critics, bon sampling, reward mdoels, etc and so the LMP abstraction enables extremely readable decomposition of difficult problems in to many calls to language models

Going from a demo to something that actually works, often means prompt engineering solutions that involve multiple calls to a language model.
By forcing a functional decomposition of the problem, ``ell`` makes it **easy to implement test-time compute leveraged techniques in a readable and modular way.**


.. image:: _static/compositionality.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%




.. code-block:: python

   import ell
   from typing import List
   
   @ell.simple(model="gpt-4o-mini", temperature=1.0, n=10)
   def write_ten_drafts(idea : str):
      """You are an adept story writer. The story should only be 3 paragraphs"""
      return f"Write a story about {idea}."

   @ell.simple(model="gpt-4o", temperature=0.1)
   def choose_the_best_draft(drafts : List[str]):
      """You are an expert fiction editor."""
      return f"Choose the best draft from the following list: {'\n'.join(drafts)}."

   drafts = write_ten_drafts(idea)

   best_draft = choose_the_best_draft(drafts) # Best of 10 sampling.

   





Every call to a language model is valuable
------------------------------------------------
Every call to a language model is worth its weight in credits. In practice, LLM invocations are used for fine tuning, distillation, k-shot prompting, reinforcement learning from human feedback, and more. A good prompt engineering system should capture these as first class concepts.

.. image:: _static/invocations.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%


In addition to storing the source code of every LMP, ``ell`` optionally saves every call to a language model locally. This allows you to generate invocaiton datasets, compare LMP outputs by version, and generally do more with the full spectrum of prompt engineering artifacts.



Complexity when you need it, simplicity when you don't
--------------------------------------------------------

Using language models is **just passing strings around, except when it's not.**

.. code-block:: python
   :emphasize-lines: 7,7

   import ell

   @ell.tool()
   def scrape_website(url : str):
      return requests.get(url).text

   @ell.complex(model="gpt-5-omni", tools=[scrape_website])
   def get_news_story(topic : str):
      return [
         ell.system("""Use the web to find a news story about the topic"""),
         ell.user(f"Find a news story about {topic}.")
      ]

   message_response = get_news_story("stock market") 
   if message_response.tool_calls:
      for tool_call in message_response.tool_calls:
         #...
   if message_response.text:
      print(message_response.text)
   if message_response.audio:
      # message_response.play_audio() supprot for multimodal outputs will work as soon as the LLM supports it
      pass

Using ``@ell.simple`` causes LMPs to yield **simple string outputs.** But when more complex or multimodal output is needed, ``@ell.complex`` can be used to yield ``Message`` objects responses from language mdoels.


Multimodality should be first class
------------------------------------------------

LLMs can process and generate various types of content, including text, images, audio, and video. Prompt engineering with these data types should be as easy as it is with text.

.. code-block:: python
   :emphasize-lines: 1,1, 9,9

   from PIL import Image
   import ell


   @ell.simple(model="gpt-4o", temperature=0.1)
   def describe_activity(image: Image.Image):
      return [
         ell.system("You are VisionGPT. Answer <5 words all lower case."),
         ell.user(["Describe what the person in the image is doing:", image])
      ]

   # Capture an image from the webcam
   describe_activity(capture_webcam_image()) # "they are holding a book"


.. image:: _static/multimodal_compressed.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%


``ell`` supports rich type coercion for multimodal inputs and outputs. You can use PIL images, audio, and other multimodal inputs inline in ``Message`` objects returned by LMPs.




Prompt engineering libraries shouldn't interfere with your workflow
--------------------------------------------------------------------

``ell`` is designed to be a lightweight and unobtrusive library. It doesn't require you to change your coding style or use special editors. 

.. image:: _static/useitanywhere_compressed.webp
   :alt: ell demonstration
   :class: rounded-image 
   :width: 100%

You can continue to use regular Python code in your IDE to define and modify your prompts, while leveraging ell's features to visualize and analyze your prompts. Migrate from langchain to ``ell`` one function at a time.

----------------------------

To get started with ``ell``, see the :doc:`Getting Started <getting_started>` section, or go onto :doc:`Installation <installation>` and get ell installed.




.. toctree::
   :maxdepth: 3
   :caption: The Basics:
   :hidden:

   Introduction <self>
   
   installation
   getting_started

.. toctree::
   :maxdepth: 3
   :caption: Core Concepts:
   :hidden:

   core_concepts/ell_simple
   core_concepts/versioning_and_storage
   core_concepts/ell_studio

   core_concepts/message_api
   core_concepts/ell_complex
   core_concepts/tool_usage
   core_concepts/structured_outputs
   core_concepts/multimodality
   core_concepts/models_and_api_clients
   core_concepts/configuration


   
.. toctree::
   :maxdepth: 1
   :caption: API Reference
   :hidden:

   reference/index
===============
Getting Started
===============

Welcome to ell, the Language Model Programming Library. This guide will walk you through creating your first Language Model Program (LMP), exploring ell's unique features, and leveraging its powerful versioning and visualization capabilities.

From Traditional API Calls to ell
---------------------------------

Let's start by comparing a traditional API call to ell's approach. Here's a simple example using the OpenAI chat completions API:

.. code-block:: python

    import openai

    openai.api_key = "your-api-key-here"

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Say hello to Sam Altman!"}
    ]

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=messages
    )

    print(response['choices'][0]['message']['content'])

Now, let's see how we can achieve the same result using ell:

.. code-block:: python

    import ell

    @ell.simple(model="gpt-4o")
    def hello(name: str):
        """You are a helpful assistant.""" # System prompt
        return f"Say hello to {name}!" # User prompt    

    greeting = hello("Sam Altman")
    print(greeting)

``ell`` simplifies prompting by encouraging you to define prompts as functional units. In this example, the ``hello`` function defines a system prompt via the docstring and a user prompt via the return string. Users of your prompt can then simply call the function with the defined arguments, rather than manually constructing the messages. This approach makes prompts more readable, maintainable, and reusable.



Understanding ``@ell.simple``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``@ell.simple`` decorator is a key concept in ell. It transforms a regular Python function into a **Language Model Program (LMP)**. Here's what's happening:

1. The function's **docstring** becomes the **system message**.
2. The **return value** of the function becomes the **user message**.
3. The decorator **handles the API call** and returns the model's response as a string.

This encapsulation allows for cleaner, more reusable code. You can now call your LMP like any other Python function.



Verbose Mode
^^^^^^^^^^^^^

To get more insight into what's happening behind the scenes, you can enable verbose mode:

.. code-block:: python

    ell.init(verbose=True)

With verbose mode enabled, you'll see detailed information about the inputs and outputs of your language model calls.


.. image:: _static/gif1.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%



Alternative Message Formats
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

While the previous example used the docstring for the system message and the return value for the user message, ell offers more flexibility. You can explicitly define messages using ``ell.system``, ``ell.user``, and ``ell.assistant``:

.. code-block:: python

    import ell

    @ell.simple(model="gpt-4o")
    def hello(name: str):
        return [
            ell.system("You are a helpful assistant."),
            ell.user(f"Say hello to {name}!"),
            ell.assistant("Hello! I'd be happy to greet Sam Altman."),
            ell.user("Great! Now do it more enthusiastically.")
        ]

    greeting = hello("Sam Altman")
    print(greeting)

This approach allows you to construct more complex conversations within your LMP. Importantly, you'll want to use this approach when you have a variable system prompt because python only allows you to have a static docstring.

Prompting as Language Model Programming
----------------------------------------

One of ell's most powerful features is its treatment of prompts as programs rather than simple strings. This approach allows you to leverage the full power of Python in your prompt engineering. Let's see how this works:

.. code-block:: python

    import ell
    import random

    def get_random_adjective():
        adjectives = ["enthusiastic", "cheerful", "warm", "friendly"]
        return random.choice(adjectives)

    @ell.simple(model="gpt-4o")
    def hello(name: str):
        """You are a helpful assistant."""
        adjective = get_random_adjective()
        return f"Say a {adjective} hello to {name}!"

    greeting = hello("Sam Altman")
    print(greeting)

In this example, our hello LMP depends on the ``get_random_adjective`` function. Each time ``hello`` is called, it generates a different adjective, creating dynamic, varied prompts.


Taking this concept further, LMPs can call other LMPs, allowing for more complex and powerful prompt engineering strategies. Let's look at an example:


.. image:: _static/compositionality.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%



.. code-block:: python

    import ell
    from typing import List

    ell.init(verbose=True)


    @ell.simple(model="gpt-4o-mini", temperature=1.0)
    def generate_story_ideas(about : str):
        """You are an expert story ideator. Only answer in a single sentence."""
        return f"Generate a story idea about {about}."

    @ell.simple(model="gpt-4o-mini", temperature=1.0)
    def write_a_draft_of_a_story(idea : str):
        """You are an adept story writer. The story should only be 3 paragraphs."""
        return f"Write a story about {idea}."

    @ell.simple(model="gpt-4o", temperature=0.1)
    def choose_the_best_draft(drafts : List[str]):
        """You are an expert fiction editor."""
        return f"Choose the best draft from the following list: {'\n'.join(drafts)}."

    @ell.simple(model="gpt-4-turbo", temperature=0.2)
    def write_a_really_good_story(about : str):
        """You are an expert novelist that writes in the style of Hemmingway. You write in lowercase."""
        # Note: You can pass in api_params to control the language model call
        # in the case n = 4 tells OpenAI to generate a batch of 4 outputs.
        ideas = generate_story_ideas(about, api_params=(dict(n=4))) 

        drafts = [write_a_draft_of_a_story(idea) for idea in ideas]

        best_draft = choose_the_best_draft(drafts)

        
        return f"Make a final revision of this story in your voice: {best_draft}."

    story = write_a_really_good_story("a dog")

In this example, ``write_a_really_good_story`` is our main LMP that calls several other LMPs to produce a high-quality story. Here's how it works:

1. First, it calls ``generate_story_ideas`` to create four different story ideas about the given topic.
2. Then, it uses ``write_a_draft_of_a_story`` to write a draft for each of these ideas.
3. Next, it uses ``choose_the_best_draft`` to select the best story from these drafts.
4. Finally, it revises the best draft in the style of Hemingway.

This approach leverages test-time compute techniques, specifically Best-of-N (BoN) sampling. By generating multiple ideas and drafts, then selecting the best one, we increase the chances of producing a high-quality output. This strategy allows us to really leverage the most out of language models in several ways:

1. **Diversity**: By generating multiple ideas and drafts, we explore a broader space of possible outputs.
2. **Quality Control**: The selection step helps filter out lower-quality outputs.
3. **Specialization**: Each step is handled by a specialized LMP, allowing for more focused and effective prompts.
4. **Iterative Improvement**: The final revision step allows for further refinement of the chosen draft.

This compositional approach to prompt engineering enables us to break down complex tasks into smaller, more manageable steps. It also allows us to apply different strategies (like varying temperature or using different models) at each stage of the process, giving us fine-grained control over the output generation.



Storing and Versioning Your Prompts
-----------------------------------

ell provides powerful versioning capabilities for your LMPs. To enable this feature, add the following line near the beginning of your script:

.. code-block:: python

    ell.init(store='./logdir', autocommit=True, verbose=True)

This line sets up a store in the ``./logdir`` directory and enables autocommit. ell will now store all your prompts and their versions in ``./logdir/ell.db``, along with a blob store for images.


Exploring Your Prompts with ell-studio
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After running your script with versioning enabled, you can explore your prompts using ell-studio. In your terminal, run:

.. code-block:: bash

    ell-studio --storage ./logdir

This command opens the ell-studio interface in your web browser. Here, you can visualize your LMPs, see their dependencies, and track changes over time.


.. image:: _static/ell_studio_better.webp
   :alt: ell demonstration
   :class: rounded-image 
   :width: 100%



Iterating and Auto-Committing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



Let's see how ell's versioning works as we iterate on our ``hello`` LMP:

Version 1:

.. code-block:: python

    import ell
    import random

    ell.init(store='./logdir', autocommit=True)

    def get_random_adjective():
        adjectives = ["enthusiastic", "cheerful", "warm", "friendly"]
        return random.choice(adjectives)

    @ell.simple(model="gpt-4o")
    def hello(name: str):
        """You are a helpful assistant."""
        adjective = get_random_adjective()
        return f"Say a {adjective} hello to {name}!"

    greeting = hello("Sam Altman")
    print(greeting)

After running this script, ell will generate an initial commit message like:

    "Initial version of hello LMP with random adjective selection."

Now, let's modify our LMP:

Version 2:

.. code-block:: python

    import ell
    import random

    ell.init(store='./logdir', autocommit=True)

    def get_random_adjective():
        adjectives = ["enthusiastic", "cheerful", "warm", "friendly", "heartfelt", "sincere"]
        return random.choice(adjectives)

    def get_random_punctuation():
        return random.choice(["!", "!!", "!!!"])

    @ell.simple(model="gpt-4o")
    def hello(name: str):
        """You are a helpful and expressive assistant."""
        adjective = get_random_adjective()
        punctuation = get_random_punctuation()
        return f"Say a {adjective} hello to {name}{punctuation}"

    greeting = hello("Sam Altman")
    print(greeting)

Running this updated script will generate a new commit message:

    "Updated hello LMP: Added more adjectives, introduced random punctuation, and modified system prompt."

ell's autocommit feature uses ``gpt-4o-mini`` to generate these commit messages automatically, providing a clear history of how your LMPs evolve.


.. image:: _static/auto_commit.png
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%


Comparing Outputs Across Versions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

One of the powerful features of ell-studio is the ability to compare outputs of your LMPs across different versions. This helps you understand how changes in your code affect the language model's responses.

For example, you can select the two versions of the ``hello`` LMP we created and compare their outputs:

.. image:: _static/compare.png
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%

This comparison might show:

Version 1 output: "Here's a warm hello to Sam Altman!"
Version 2 output: "Here's a heartfelt hello to Sam Altman!!!"

By visualizing these differences, you can quickly assess the impact of your changes and make informed decisions about your prompt engineering process.

What's Next?
------------

Now that you've created your first LMP, explored versioning, and learned about ell-studio, there's much more to discover:

- ``@ell.complex``: For advanced use cases involving tool usage, structured outputs, and the full message API.
- Multimodal inputs and outputs: Work with images, videos, and audio in your LMPs.
- API clients and models: Explore various language models and APIs supported by ell.
- Designing effective Language Model Programs: Discover best practices for creating robust and efficient LMPs.
- Tutorials: Check out in-depth tutorials for real-world applications of ell.
=============
ell package
=============

.. note::
   This is coming soon, be sure to read the source code.

Automatically generated API reference for ``ell``.


.. automodule:: ell
   :members:
   :undoc-members:
   :show-inheritance:


.. toctree::
   :maxdepth: 2
   :caption: API Reference

   self===========
Messages
===========

Messages are the fundamental unit of interaction with chat-based language models. They consist of a role and some form of content. For text-only language models, the content is typically just a string. However, with multimodal language models that can process images, audio, text, and other modalities, the content object becomes more complex.

In practice, the content that a language model can consume forms a markup language, where there are different content blocks for text, images, audio, tool use, and so on.

Challenges with LLM APIs
------------------------

The potential complexity of a message object has led language model APIs to establish message specifications that are often quite pedantic, even when users only want to pass around simple types like strings or images. This issue is compounded by the fact that most language model APIs are automatically generated using tools like Stainless, which take an API spec and build multi-language client-side API bindings. Because these APIs are automatically generated, they can't be optimized for user-friendliness.

For example, many prompt engineering libraries exist primarily to solve the inconvenience of indexing into responses from APIs like OpenAI's. This complexity in both specifying prompts and handling responses can make working with language models unnecessarily cumbersome for developers.

.. code-block:: python

    result : str = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of the moon?"}
        ]
    )["choices"][0]["message"]["content"] # hughkguht this line

    result : str my_prompt_engineering_library("prompt")

Likewise, the specification of prompts themselves is also quite cumbersome. Because language model provider API client bindings are often automatically generated, they lack developer-friendly features. As a result, users need to be as verbose and pedantic as possible when constructing prompts. Consider the complexity of passing an input with both text and images to a language model API:

.. code-block:: python

    result : str = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [ # highlight these lines 
                {"type": "text", "text": "What is the capital of the moon?"},
                {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}} 
            ]}
        ]
    )["choices"][0]["message"]["content"] 

In essence, the user has to explicitly specify two different content blocks and their types, even though these types are implicit and could be inferred. This is because the language bindings use typed dictionaries and validators generated by tools like Stainless or similar co-generation tools. While not inherently wrong, this approach creates a gap in developer experience, making the code less readable and more cumbersome to work with.

This leads us to a core philosophy in ell:

    "Using language models is just passing around strings, except when it's not."

Users should be able to specify the minimal amount of complexity necessary for the data they want to pass to a language model. To achieve this, we've drawn inspiration from machine learning and scientific computing libraries like TensorFlow, PyTorch, and NumPy to create a new type of message API. In this API, type coercion and implicit inference are key features that enhance the developer experience.

The ell Message API
-------------------

Our API centers around two key objects: Messages and ContentBlocks.

.. autopydantic_model:: ell.Message()
    :exclude-members: text, content, role, audios, images, parsed, text_only, tool_calls, tool_results, call_tools_and_collect_as_message, to_openai_message
    :model-show-json: false

.. autopydantic_model:: ell.ContentBlock()
    :members: text, image, audio, tool_call, parsed, tool_result
    :exclude-members: audio, image, parsed, text, tool_call, tool_result, check_single_non_null, to_openai_content_block, serialize_image, validate_image, validate_image, coerce, type
    :model-show-validator-members: false
    :model-show-config-summary: false
    :model-show-validator-summary: false
    :model-show-json: false

Solving the construction problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Message and ContentBlock objects solve the problem of pedantic construction by incorporating type coercion directly into their constructors.

Consider constructing a message that contains both text and an image. Traditionally, you might need to create a Message with a role and two ContentBlocks - one for text and one for an image:

.. code-block:: python

    from ell import Message, ContentBlock

    message = Message(
        role="user",
        content=[
            ContentBlock(text="What is the capital of the moon?"),
            ContentBlock(image=some_PIL_image_object)
        ]
    )

However, the Message object can infer the types of content blocks within it. This allows for a more concise construction:

.. code-block:: python

    message = Message(
        role="user",
        content=["What is the capital of the moon?", some_PIL_image_object]
    )

Furthermore, if a message contains only one type of content (for example, just an image), we also support shape coercion:

.. code-block:: python

    message = Message(
        role="user",
        content=some_PIL_image_object
    )


Coercion is an important concept in ell, and you can read more about it in the Content Block Coercion API reference page.

Common roles
^^^^^^^^^^^^

.. code-block:: python

    message = ell.user(["What is the capital of the moon?", some_PIL_image_object])

Ell's message API provides several common helper functions for constructing messages with specific roles in language model APIs. These functions essentially partially compose the Message constructor with a specific role. All of the type coercion and convenient functionality from before is automatically handled.

.. autofunction:: ell.system
.. autofunction:: ell.user
.. autofunction:: ell.assistant

Solving the parsing problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Complex message structures shouldn't mean complex interactions. Drawing inspiration from rich HTML APIs and JavaScript's document selector API, as well as BeautifulSoup's helper functions for extracting text from HTML documents, we've built convenient functions for interacting with the contents of a message.


To understand why this approach is necessary, let's examine how we might parse output from the traditional OpenAI API if the model had multimodal capabilities. This example will illustrate the complexity of handling various content types without a unified message structure.

.. code-block:: python

    from ell import Message, ContentBlock
    import openai

    # Assume we have a response from a multimodal language model
    response = openai.ChatCompletion.create(
        model="gpt-5-omni",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "text", "text": "Draw me a sketch version of this image"},
                {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
            ]}
        ]
    )

    # Access the message content from the OpenAI response
    message_content = response.choices[0].message.content

    # Check for different types of content in the traditional OpenAI API format
    has_image = any(content.get('type') == 'image_url' for content in message_content if isinstance(content, dict))
    has_text = any(content.get('type') == 'text' for content in message_content if isinstance(content, dict))
    has_tool_call = 'function_call' in response.choices[0].message

    if has_image:
        image_content = [content['image_url']['url'] for content in message_content if isinstance(content, dict) and content.get('type') == 'image_url']
        show(image_content[0])
    if has_text:
        # Extract text content
        text_content = [content['text'] for content in message_content if isinstance(content, dict) and content.get('type') == 'text']
        print("".joitext_content[0])
    if has_tool_call:
        print("The message contains a tool call.")


Now let's see how we can do the same thing using ell's message API. In the following example, we'll use ell's ``@ell.complex`` decorator which is similar to ``@ell.simple``. However, instead of returning a string after calling the language model program, it returns a Message object representing the response from the language model. This allows you to have language model responses with multimodal output, including structured and tool call output. You can learn more about this in the :doc:`@ell.complex <ell_complex>` section.

.. code-block:: python

    import ell

    @ell.complex(model="gpt-5-omni")
    def draw_sketch(image: PILImage.Image):
        return [
            ell.system("You are a helpful assistant."),
            ell.user(["Draw me a sketch version of this image", image]),
        ]

    response = draw_sketch(some_PIL_image_object)

    if response.images:
        show(response.images[0])
    if response.text:
        print(response.text)
    if response.tool_calls:
        print("The message contains a tool call.")
        
        
The following conevnience functions and properties are available on a Message object:


.. autoproperty:: ell.Message.text
.. autoproperty:: ell.Message.text_only
.. autoproperty:: ell.Message.tool_calls
.. autoproperty:: ell.Message.tool_results
.. autoproperty:: ell.Message.parsed
.. autoproperty:: ell.Message.images
.. autoproperty:: ell.Message.audios
.. automethod:: ell.Message.call_tools_and_collect_as_message

.. XXX put all the content block types here.=================================================
Studio
=================================================

In the previous chapter, we explored ell's powerful versioning and tracing capabilities. These features provide a solid foundation for managing and analyzing your Language Model Programs (LMPs). However, to truly leverage the full potential of this data, we need a tool that can visualize and interpret it effectively. This is where Studio comes in.



.. image:: ../_static/ell_studio_better.webp
   :alt: ell demonstration
   :class: rounded-image 
   :width: 100%



Studio is a powerful, open-source visualization and analysis tool that complements ell's versioning and tracing capabilities. It runs locally on your machine, ensuring data privacy and security. Studio provides an intuitive interface for exploring LMPs, their versions, and interactions, transforming the abstract data collected by ell into actionable insights.

With Studio, you can visualize the evolution of your LMPs over time, analyze the performance and behavior of your prompts, debug complex interactions between multiple LMPs, and collaborate more effectively with your team on prompt engineering tasks. In essence, Studio turns the wealth of data collected by ell's versioning and tracing systems into a powerful asset for prompt engineering, all while keeping your data local and under your control.

Launching Studio
--------------------

To start using Studio, run the following command in your terminal:

.. code-block:: bash

    ell-studio --storage ./logdir

Then go to `http://localhost:5555 <http://localhost:5555>`_ to access the Studio interface.

This command opens the Studio interface in your web browser, using the data stored in the specified directory (which should be the same directory you specified when initializing ell with `ell.init(store='./logdir')`). Since Studio runs locally, you can be assured that your sensitive prompt data never leaves your machine.

Key Features of Studio
--------------------------

LMP Visualization
^^^^^^^^^^^^^^^^^

Studio offers a comprehensive visual representation of your LMPs and their dependencies. This feature allows you to visualize the structure of complex, multi-LMP programs, understand the interactions and dependencies between different LMPs, and identify potential bottlenecks or areas for optimization in your LMP architecture.

.. image:: ../_static/compositionality.webp
   :alt: LMP dependency visualization
   :class: rounded-image invertible-image
   :width: 100%

Version History and Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Leveraging ell's automatic versioning capabilities, Studio provides a complete version history for each LMP. You can perform side-by-side comparisons of different LMP versions and view auto-generated commit messages explaining changes between versions. This feature is particularly useful for tracking the evolution of your LMPs over time and understanding the impact of specific changes.

.. image:: ../_static/auto_commit.png
   :alt: Version history and commit messages
   :class: rounded-image invertible-image
   :width: 100%

Invocation Analysis
^^^^^^^^^^^^^^^^^^^

Studio offers detailed insights into each LMP invocation, including input parameters, output results, execution time, and token usage metrics. It also provides tracing information showing data flow between LMPs. This level of detail allows for in-depth analysis of LMP performance and behavior.

.. image:: ../_static/invocations.webp
   :alt: Invocation analysis
   :class: rounded-image invertible-image
   :width: 100%

Performance Metrics
^^^^^^^^^^^^^^^^^^^

To help optimize your LMPs, Studio provides various performance metrics such as token usage over time, execution time trends, and frequency of invocations for each LMP. These metrics can be invaluable in identifying performance bottlenecks and areas for improvement.

LMP Viewer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Studio includes a built-in code viewer that allows you to examine the source code of your LMPs. You can compare different versions side-by-side and access the full context of each LMP quickly and easily.



.. Leveraging Studio in Prompt Engineering
.. -------------------------------------------

.. Studio transforms prompt engineering from a black box process into a data-driven, visual, and collaborative endeavor. Throughout your prompt engineering workflow, you can use Studio for iterative development, debugging, performance optimization, and collaboration.

.. During iterative development, Studio helps you track changes over time, identify which modifications led to improvements, and revert to previous versions if needed. The version history and comparison features are particularly useful in this process.

.. For debugging, the invocation analysis allows you to pinpoint exactly what input led to unexpected output. You can trace data flow to identify where problems might be originating and compare problematic invocations with successful ones. The detailed invocation analysis and tracing capabilities of Studio make debugging a more straightforward process.

.. When it comes to performance optimization, Studio's metrics help you analyze token usage and execution time to find inefficiencies. You can identify frequently used LMPs that might benefit from caching or optimization, and compare different versions to see which changes had the most significant impact on performance.

.. Studio also facilitates collaboration on prompt engineering projects. You can share visualizations of complex LMP structures with team members, use version history and commit messages to communicate changes, and provide a central point of reference for discussing and improving LMPs.

.. Advanced Features
.. -----------------

.. Multimodal Visualization
.. ^^^^^^^^^^^^^^^^^^^^^^^^

.. For LMPs that work with images, audio, or other non-text data, Studio provides previews of input and output media files and visualization of how multimodal data flows between LMPs. This feature is particularly useful when working with more complex, multimodal LMPs.

.. Custom Metrics and Dashboards
.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. Studio allows you to define and track custom metrics relevant to your specific use case and create dashboards to monitor the most important aspects of your LMPs. These custom metrics and dashboards can be tailored to your project's specific goals and requirements.

.. Integration with External Tools
.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. Studio can be integrated with other development tools, allowing you to export data for further analysis in other platforms and import external metrics or annotations to enrich your LMP analysis. This integration capability ensures that Studio can fit seamlessly into your existing development workflow.

.. Conclusion
.. ----------

.. Studio is a powerful tool that brings transparency and insight to the prompt engineering process. By providing deep insights into your LMPs, their versions, and their performance, Studio empowers you to create more effective, efficient, and reliable language model programs.

.. As you continue to work with ell, integrating Studio into your workflow can significantly enhance your prompt engineering capabilities. Its comprehensive features will help you navigate the complexities of prompt engineering, leading to better outcomes and a deeper understanding of your language model interactions.

===================
Structured Outputs
===================


Structured outputs are essential for ensuring that language model responses are both controlled and predictable. By defining a clear schema for the expected output, we can leverage the power of language models to generate responses that adhere to specific formats and constraints.

Consider the following example, which demonstrates how to use Pydantic models to define structured outputs in ell:

.. code-block:: python

   from pydantic import BaseModel, Field

   class MovieReview(BaseModel):
       title: str = Field(description="The title of the movie")
       rating: int = Field(description="The rating of the movie out of 10")
       summary: str = Field(description="A brief summary of the movie")

   @ell.complex(model="gpt-4o-2024-08-06", response_format=MovieReview)
   def generate_movie_review(movie: str) -> MovieReview:
       """You are a movie review generator. Given the name of a movie, you need to return a structured review."""
       return f"generate a review for the movie {movie}"

By defining the `MovieReview` model, we ensure that the output of the `generate_movie_review` function adheres to a specific structure, making it easier to parse and utilize in downstream applications. This approach not only enhances the reliability of the generated content but also simplifies the integration of language model outputs into larger systems.

Once we have defined and generated structured outputs, we can easily access and manipulate the data within them. Let's continue with our movie review example to demonstrate how to work with structured outputs:

.. code-block:: python

   # Generate a movie review
   message = generate_movie_review("The Matrix")
   review = message.parsed

   # Access individual fields
   print(f"Movie Title: {review.title}")
   print(f"Rating: {review.rating}/10")
   print(f"Summary: {review.summary}")

In this example, we first generate a movie review using our `generate_movie_review` function. We can then access individual fields of the structured output directly, as shown in the first part of the code.




.. note::
   Structured outputs using Pydantic models are currently only available for the ``gpt-4o-2024-08-06`` model. For other models, you'll need to manually prompt the model and enable JSON mode to achieve similar functionality. 

   We purposefully chose to not opinionate prompting for other non-native json models because each prompt should be customized to the specific model and situation. For example if you want to get gpt-3.5-turbo to return json you should explicitly allow it by prompting the model to do so:

   .. code-block:: python

      class MovieReview(BaseModel):
          title: str = Field(description="The title of the movie")
          rating: int = Field(description="The rating of the movie out of 10")
          summary: str = Field(description="A brief summary of the movie")

      @ell.simple(model="gpt-3.5-turbo")
      def generate_movie_review_manual(movie: str):
          return [
              ell.system(f"""You are a movie review generator. Given the name of a movie, you need to return a structured review in JSON format.

      You must absolutely respond in this format with no exceptions.
      {MovieReview.model_json_schema()}
      """),
              ell.user("Review the movie: {movie}"),
          ]

      # parser support coming soon!
      unparsed = generate_movie_review_manual("The Matrix")
      parsed = MovieReview.model_validate_json(unparsed)

========================
Models & API Clients
========================

In language model programming, the relationship between models and API clients is crucial. ell provides a robust framework for managing this relationship, offering various ways to specify clients for models, register custom models, and leverage default configurations.

Model Registration and Default Clients
--------------------------------------

ell automatically registers numerous models from providers like OpenAI, Anthropic, Cohere, and Groq upon initialization. This allows you to use models without explicitly specifying a client. 

If no client is found for a model, ell falls back to a default OpenAI client. This enables the utilization of newly released models without updating ell for new model registrations. If the fallback fails because the model is not available in the OpenAI API, you can register your own client for the model using the `ell.config.register_model` method or specify a client when calling the language model program below


Specifying Clients for Models
-----------------------------

ell offers multiple methods to specify clients for models:

1. Decorator-level Client Specification:

.. code-block:: python

    import ell
    import openai

    client = openai.Client(api_key="your-api-key")

    @ell.simple(model="gpt-next", client=client)
    def my_lmp(prompt: str):
        return f"Respond to: {prompt}"

2. Function Call-level Client Specification:

.. code-block:: python

    result = my_lmp("Hello, world!", client=another_client)

3. Global Client Registration:

.. code-block:: python

    ell.config.register_model("gpt-next", my_custom_client)

Custom Model Registration
-------------------------

For custom or newer models, ell provides a straightforward registration method:

.. code-block:: python

    import ell
    import my_custom_client

    ell.config.register_model("my-custom-model", my_custom_client)

=============
Configuration
=============

ell provides various configuration options to customize its behavior.

.. autofunction:: ell.init

This ``init`` function is a convenience function that sets up the configuration for ell. It is a thin wrapper around the ``Config`` class, which is a Pydantic model.

You can modify the global configuration using the ``ell.config`` object which is an instance of ``Config``:

.. autopydantic_model:: ell.Config
    :members:
    :exclude-members: default_client, registry, store
    :model-show-json: false
    :model-show-validator-members: false
    :model-show-config-summary: false
    :model-show-field-summary: false
    :model-show-validator-summary: false===========
@ell.simple
===========


The core unit of prompt engineering in ell is the ``@ell.simple`` decorator. This decorator transforms a function that provides system and user prompts into a callable object. When invoked, this callable sends the provided prompts to a language model and returns the model's response. 

The development of ``@ell.simple`` is driven by several important objectives:

- Improve readability and usabiltiy of prompt engineering code.
- Force a functional decomposition of prompt systems into reusable components.
- Enable versioning, serialization, and tracking of prompts over time


Usage
-----

The ``@ell.simple`` decorator can be used in two main ways:

1. Using the docstring as the system prompt, and the return value as the user message:

   .. code-block:: python

      @ell.simple(model="gpt-4")
      def hello(name: str):
          """You are a helpful assistant."""
          return f"Say hello to {name}!"

2. Explicitly defining messages:

   .. code-block:: python

      @ell.simple(model="gpt-4")
      def hello(name: str):
          return [
              ell.system("You are a helpful assistant."),
              ell.user(f"Say hello to {name}!")
          ]

.. note:: Messages in ell are not the same as the dictionary messages used in the OpenAI API. ell's Message API provides a more intuitive and flexible way to construct and manipulate messages. You can read more about ell's Message API and type coercion in the :doc:`message_api` page.


Invoking an ``ell.simple`` LMP
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To use the decorated function, we can call it as a normal function. However, instead of receiving the typical return value, we will receive the result of passing the system and user prompts directly to the model specified in the decorator constructor, in this case GPT-4.

.. code-block:: python

    >>> hello("world")
    'Hello, world!'

As you can see from this example, the return type of an ``ell.simple`` LMP is a string. This is to optimize for readability and usability, as most invocations of language models revolve around passing strings around. Additional metadata is only needed occasionally.

Therefore, we have two decorators within the ell framework:

1. ``@ell.simple``: Returns simple strings, as shown here.

2. ``@ell.complex``: Returns message objects containing all of the typical message API metadata and additional helper functions for interacting with multimodal output data. You can read more about this in the :doc:`ell_complex` page.


Variable system prompts
^^^^^^^^^^^^^^^^^^^^^^^

One of the challenges with specifying the system prompt in the docstring of a language model program is that if you want to use variable system prompts, Python will no longer treat the string literal at the top of the function as a docstring. For example:

.. code-block:: python

    def my_func(var : int):
        f"""my variable doc string for my_func. {var}"""
        pass

.. code-block:: python

    >>> my_func.__doc__
    None

This behavior makes sense because a function's docstring should not change during execution and should be extractable through static analysis.

To address this issue with ``@ell.simple``, you need to use the second method of defining an ``ell.simple`` language model program by creating a function that returns a list of messages (see :doc:`message_api` for more details).

.. code-block:: python

    @ell.simple(model="gpt-4")
    def my_func(name : str, var : int):
        return [
            ell.system(f"You are a helpful assistant. {var}"),
            ell.user(f"Say hello to {name}!")
        ]

With this approach, ell will ignore the docstring of ``my_func`` and instead supply the messages returned by the function to the language model API.

Passing parameters to an LLM API
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
One of the most convenient functions of the ``@ell.simple`` decorator is that you can easily pass parameters to an LLM API, both at definition time and runtime. For example, models within the OpenAI API have parameters like ``temperature``, ``max_tokens``, stop tokens, and ``logit_bias``. Due to how ``@ell.simple`` works, you can simply specify these in the decorator as keyword arguments.

.. code-block:: python

    @ell.simple(model="gpt-4", temperature=0.5, max_tokens=100, stop=["."])
    def hello(name: str):
        """You are a helpful assistant."""
        return f"Hey there {name}!"
        
Likewise, if you want to modify those parameters for a particular invocation of that prompt, you simply pass them in as ``api_params`` keyword arguments to the function when calling it. For example:

.. code-block:: python

    >>> hello("world", api_params=dict(temperature=0.7))
    'Hey there world!'


Multiple outputs (n>1)
~~~~~~~~~~~~~~~~~~~~~~~
As is often important in prompt engineering to leverage test-time compute, many language model APIs allow you to specify a count parameter, usually 'n', which will generate several outputs from the language model given a particular prompt.

In the OpenAI API, for example, this is actually quite cumbersome because the API specification separates different completions into 'choices' objects. For example:

.. code-block:: python

    response = openai.Completion.create(
        model="gpt-4",
        prompt="Say hello to everyone",
        n=2
    )
    
    r1 = response.choices[0].text
    r2 = response.choices[1].text

In the spirit of simplicity, we've designed it to automatically coerce the return type into the correct shape, similar to NumPy and PyTorch. This means that when you call an ``ell.simple`` language model program with ``n`` greater than one, instead of returning a string, it returns a list of strings.

.. code-block:: python

    @ell.simple(model="gpt-4", n=2)
    def hello(name: str):
        """You are a helpful assistant."""
        return f"Say hello to {name}!"

.. code-block:: python

    >>> hello("world")
    ['Hey there world!', 'Hi, world.']

Similarly, this behavior applies when using runtime ``api_params`` to specify multiple outputs.

.. code-block:: python

    >>> hello("world", api_params=dict(n=3))
    ['Hey there world!', 'Hi, world.', 'Hello, world!']


.. note:: In the future, we may modify this interface as preserving the ``api_params`` keyword in its current form could potentially lead to conflicts with user-defined functions. However, during the beta phase, we are closely monitoring for feedback and will make adjustments based on user experiences and needs.


Multimodal inputs
^^^^^^^^^^^^^^^^^
``@ell.simple`` supports multimodal inputs, allowing you to easily work with both text and images in your language model programs. This is particularly useful for models with vision capabilities, such as GPT-4 with vision.

Here's an example of how to use ``@ell.simple`` with multimodal inputs:

.. code-block:: python

    from PIL import Image
    import ell
    from ell.types.message import ImageContent

    @ell.simple(model="gpt-4-vision-preview")
    def describe_image(image: Image.Image):
        return [
            ell.system("You are a helpful assistant that describes images."),
            ell.user(["What's in this image?", image])
            # Or ell.user(["What's in this image?", ImageContent(url=image_url, detail="low")])
        ]

    # Usage with PIL Image
    image = Image.open("path/to/your/image.jpg")
    description = describe_image(image)
    print(description)  # This will print a text description of the image


In these examples, the ``describe_image`` function takes a PIL Image object as input, while ``describe_image_url`` takes a string URL. The ``ell.user`` message combines both text and image inputs. ``@ell.simple`` automatically handles the conversion of the PIL Image object or ImageContent into the appropriate format for the language model.

This approach simplifies working with multimodal inputs, allowing you to focus on your application logic rather than the intricacies of API payloads.

.. note:: Not all language model providers support image URLs. For example, as of the current version, Anthropic's models do not support image URLs. Always check the capabilities and requirements of your chosen language model provider when working with multimodal inputs.

.. warning:: While ``@ell.simple`` supports multimodal inputs, it is designed to return text-only outputs. For handling multimodal outputs (such as generated images or audio), you need to use ``@ell.complex``. Please refer to the :doc:`ell_complex` documentation for more information on working with multimodal outputs.



What about multiturn conversations, tools, structured outputs, and other features?
----------------------------------------------------------------------------------

While ``@ell.simple`` is great for straightforward text-based interactions with language models, there are scenarios where you might need more complex functionality. For instance, you may want to work with multiturn conversations, utilize tools, generate structured outputs, or handle multimodal content beyond just text.

In such cases, you'll need an LMP that can return rich ``Message`` objects instead of just strings. This is where ``@ell.complex`` comes into play. The ``@ell.complex`` decorator provides enhanced capabilities for more sophisticated interactions with language models.

For more information on how to use ``@ell.complex`` and its advanced features, please refer to the :doc:`ell_complex` documentation.


Reference
---------

.. autofunction:: ell.simple
==============
Multimodality
==============

As the capabilities of language models continue to expand, so too does the need for frameworks that can seamlessly handle multiple modalities of input and output. ell rises to this challenge by providing robust support for multimodal interactions, allowing developers to work with text, images, audio, and more within a unified framework.

The Evolution of Multimodal Interactions
----------------------------------------

Traditionally, working with language models has been primarily text-based. However, the landscape is rapidly changing. Models like GPT-4 with vision capabilities, or DALL-E for image generation, have opened up new possibilities for multimodal applications. This shift presents both opportunities and challenges for developers.

Consider the complexity of constructing a prompt that includes both text and an image using a traditional API:

.. code-block:: python

    result = openai.ChatCompletion.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What's in this image?"},
                    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
                ]
            }
        ]
    )

This approach, while functional, is verbose and can become unwieldy as the complexity of inputs increases. It doesn't align well with the natural flow of programming and can make code less readable and more error-prone.

ell's Approach to Multimodality
-------------------------------

ell addresses these challenges by treating multimodal inputs and outputs as first-class citizens within its framework. Let's explore how ell simplifies working with multiple modalities:

1. Simplified Input Construction

ell's Message and ContentBlock objects, which we explored in the Message API chapter, shine when it comes to multimodal inputs. They allow for intuitive construction of complex prompts:

.. code-block:: python

    from PIL import Image
    import ell

    @ell.simple(model="gpt-4-vision-preview")
    def describe_image(image: Image.Image):
        return [
            ell.system("You are a helpful assistant that describes images."),
            ell.user(["What's in this image?", image])
        ]

    result = describe_image(some_pil_image) # 'There's a cat in the image'

Notice how ell automatically handles the conversion of the PIL Image object into the appropriate format for the language model. This abstraction allows developers to focus on their application logic rather than the intricacies of API payloads.

ell also supports working with image URLs, making it easy to reference images hosted online:

.. code-block:: python

    from ell.types.message import ImageContent

    @ell.simple(model="gpt-4o-2024-08-06")
    def describe_image_from_url(image_url: str):
        return [
            ell.system("You are a helpful assistant that describes images."),
            ell.user(["What's in this image?", ImageContent(url=image_url, detail="low")])
        ]

    result = describe_image_from_url("https://example.com/cat.jpg")

This flexibility allows developers to work with both local images and remote image URLs seamlessly within the ell framework.

2. Flexible Output Handling

Just as ell simplifies input construction, it also provides flexible ways to handle multimodal outputs. The Message object returned by ``@ell.complex`` decorators offers convenient properties for accessing different types of content:

.. code-block:: python

    @ell.complex(model="gpt-5-omni")
    def generate_audiovisual_novel(topic : str):
        return [
            ell.system("You are a helpful assistant that can generate audiovisual novels. Output images, text, and audio simultaneously."),
            ell.user("Generate a novel on the topic of {topic}")
        ]

.. code-block:: python

    >>> result = generate_audiovisual_novel("A pirate adventure")
    Message(role="assistant", content=[
        ContentBlock(type="text", text="Chapter 1: The Treasure Map"),
        ContentBlock(type="image", image=PIL.Image.Image(...)),
        ContentBlock(type="text", text="The crew of the ship set sail on a quest to find the lost treasure of the pirate king. They must navigate treacherous waters, avoid the wrath of the sea monsters, and outsmart the other pirates who are also searching for the treasure."),
        ContentBlock(type="audio", audio=np.array([...])),
    ])

.. code-block:: python

    if result.images:
        for img in result.images:
            display(img)
    
    if result.text:
        print(result.text)

    if result.audios:
        for audio in result.audios:
            play(audio)

This approach allows for intuitive interaction with complex, multimodal outputs without the need for extensive parsing or type checking.

3. Seamless Integration with Python Ecosystem

ell's design philosophy extends to its integration with popular Python libraries for handling different media types. For instance, it works seamlessly with PIL for images, making it easy to preprocess or postprocess visual data:

.. code-block:: python

    from PIL import Image, ImageEnhance

    def enhance_image(image: Image.Image) -> Image.Image:
        enhancer = ImageEnhance.Contrast(image)
        return enhancer.enhance(1.5)

    @ell.complex(model="gpt-4-vision-preview")
    def analyze_enhanced_image(image: Image.Image):
        enhanced = enhance_image(image)
        return [
            ell.system("Analyze the enhanced image and describe any notable features."),
            ell.user(enhanced)
        ]

This example demonstrates how ell allows for the seamless integration of image processing techniques within the language model workflow.

The Power of Multimodal Composition
-----------------------------------

One of the most powerful aspects of ell's multimodal support is the ability to compose complex workflows that involve multiple modalities. Let's consider a more advanced example:

.. code-block:: python

    @ell.simple(model="gpt-4o")
    def generate_image_caption(image: Image.Image):
        return [
            ell.system("Generate a concise, engaging caption for the image."),
            ell.user(image)
        ]

    @ell.complex(model="gpt-4-audio")
    def text_to_speech(text: str):
        return [
            ell.system("Convert the following text to speech."),
            ell.user(text)
        ]

    @ell.complex(model="gpt-4")
    def create_social_media_post(image: Image.Image):
        caption = generate_image_caption(image)
        audio = text_to_speech(caption)
        
        return [
            ell.system("Create a social media post using the provided image, caption, and audio."),
            ell.user([
                "Image:", image,
                "Caption:", caption,
                "Audio:", audio.audios[0]
            ])
        ]

    post = create_social_media_post(some_image)

In this example, we've created a workflow that takes an image, generates a caption for it, converts that caption to speech, and then combines all these elements into a social media post. ell's multimodal support makes this complex interaction feel natural and intuitive.

Multimodality in ell isn't just a feature; it's a fundamental design principle that reflects the evolving landscape of AI and machine learning. By providing a unified, intuitive interface for working with various types of data, ell empowers developers to create sophisticated, multimodal applications with ease.
=================================================
Versioning & Tracing
=================================================

Prompt Engineering is the process of rapidly iterating on the set of system, user, and pre-packaged assistant messages sent to a language model. The goal is to maximize some explicit or implied objective function. In an ideal scientific scenario, we would have reward models or metrics that could automatically assess the quality of prompts. One would simply modify the text or formatting of the sent messages to maximize this objective.

However, the reality of this process is much messier. Often, a prompt engineer will work on a few examples for the language model program they're trying to develop, tweaking the prompt slightly over time, testing and hoping that the resulting outputs seem better in practice. This process comes with several issues:

1. It's often unclear if a change in a prompt will uniformly improve the quality of a language model program.
2. Sometimes regressions are introduced, unknown to the prompt engineer, due to dependencies elsewhere in the codebase.
3. The process of testing different hypotheses and then reverting those tests often involves using the undo/redo shortcuts in the editor of choice, which is not ideal for tracking changes.


Checkpointing prompts
----------------------

A solution to this problem can be found by drawing analogies to the training process in machine learning. Prompt engineering, in essence, is a form of parameter search. We modify a model over time with local updates, aiming to maximize or minimize some global objective function.

In machine learning, this process is known as the training loop. Each instance of the model's parameters is called a checkpoint. These checkpoints are periodically saved and evaluated for quality. If a hyperparameter change leads to a failure in the training process, practitioners can quickly revert to a previous checkpoint, much like version control in software engineering.


However, versioning or checkpointing prompts during the prompt engineering process is cumbersome with standard language model API calls or current frameworks. Prompt engineers often resort to inefficient methods:

- Checking in prompt code to version control systems like git for every minor change during the iterative prompt engineering process
- Storing commit hashes alongside outputs for comparison
- Saving prompts and outputs to text files

These approaches are highly cumbersome and go against typical version control workflows in software development. Some prompt engineering frameworks offer versioning, but they often require the use of pre-built IDEs or specific naming conventions. This approach doesn't align well with real-world LLM applications, where calls are often scattered throughout a codebase.


A key feature of ``ell`` is its behind-the-scenes version control system for language model programs. This system allows for comparison, visualization, and storage of prompts as the codebase evolves, both in production and development settings. Importantly, it requires no changes to the prompt engineer's workflow.

Serializing prompts via lexical closures
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


This automatic versioning is possible because ell treats prompts as discrete functional units called language model programs. By encapsulating the prompt within a function, we can use static and dynamic analysis tools to extract the source code of a prompt program and all its lexical dependencies at any point in time. This approach captures the exact set of source code needed to reproduce the prompt.


Consider the following function embedded in a large code base.

.. code-block:: python

    from myother_module import CONSTANT
    def other_code():
        print("hello")

    def some_other_function():
        return "to bob"

    @ell.simple(model="gpt-4o")
    def hi():
        """You are a helpful assistant"""
        return f"say hi {some_other_function()} {CONSTANT} times."

    def some_other_code():
        return "some other code"

What does it mean to serialize and version the LMP `hi` above? A first approach might be to simply capture the source code of the function body and its signature. 

.. code-block:: python

    @ell.simple(model="gpt-4o")
    def hi():
        """You are a helpful assistant"""
        return f"say hi {some_other_function()} {CONSTANT} times."

However, this approach isn't quite sufficient. If the dependency `some_other_function` changes, the language model program `hi` has fundamentally changed as well. Consequently, all the outputs you expect to see when calling it would also change. Fortunately, the solution is to compute the lexical closure. The lexical closure of a function is essentially its source code along with the source of every global and free variable that it depends on. For example:

.. code-block:: python

    >>> lexical_closure(hi) 
    '''
    CONSTANT = 6

    def some_other_function():
        return "to bob"

    @ell.simple(model="gpt-4o")
    def hi():
        """You are a helpful assistant"""
        return f"say hi {some_other_function()} {CONSTANT} times."
    '''

Full closure can be computed through static analysis by inspecting the Abstract Syntax Tree (AST) of the function and all of its bound globals. This process recursively enumerates dependencies to compute a minimal set of source code that would enable you to reproduce the function. For brevity, we can ignore system and user libraries that were installed by package managers, as these are typically considered part of the execution environment rather than the function's specific closure.

Constructing a dependency graph
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition, when a language model program depends on another prompt (i.e., when one language model program calls another), the dependent prompt will automatically appear within the lexical closure of the calling prompt. This allows us to construct a computation graph that illustrates how language model programs depend on one another to execute, effectively leveraging test-time compute. This graph provides a clear visualization of the relationships and dependencies between different prompts in a complex language model program.

.. image:: ../_static/compositionality.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%

.. code-block:: python
    
    import ell
    from typing import List


    @ell.simple(model="gpt-4o-mini", temperature=1.0)
    def generate_story_ideas(about : str):
        """You are an expert story ideator. Only answer in a single sentence."""
        return f"Generate a story idea about {about}."

    @ell.simple(model="gpt-4o-mini", temperature=1.0)
    def write_a_draft_of_a_story(idea : str):
        """You are an adept story writer. The story should only be 3 paragraphs."""
        return f"Write a story about {idea}."

    @ell.simple(model="gpt-4o", temperature=0.1)
    def choose_the_best_draft(drafts : List[str]):
        """You are an expert fiction editor."""
        return f"Choose the best draft from the following list: {'\n'.join(drafts)}."

    @ell.simple(model="gpt-4-turbo", temperature=0.2)
    def write_a_really_good_story(about : str):
        """You are an expert novelist that writes in the style of Hemmingway. You write in lowercase."""
        # Note: You can pass in api_params to control the language model call
        # in the case n = 4 tells OpenAI to generate a batch of 4 outputs.
        ideas = generate_story_ideas(about, api_params=(dict(n=4))) 

        drafts = [write_a_draft_of_a_story(idea) for idea in ideas]

        best_draft = choose_the_best_draft(drafts)

        
        return f"Make a final revision of this story in your voice: {best_draft}."

    story = write_a_really_good_story("a dog")

Versioning
----------

With the ability to checkpoint and serialize prompts, we can now facilitate a key promise of a useful prompt engineering library: automatic versioning.

Prompt versioning comes in two flavors: automatic versioning during the prompt engineering process, and archival versioning in storage during production deployments. The former is important for the reasons previously mentioned; as a prompt engineer changes and tunes the prompt over time, they may often revert to previous versions or need to compare across them. The latter is crucial for debugging and regression checks of production deployments, as well as the creation of large-scale fine-tuning and comparison datasets. ell is designed with both of these in mind.

In designing ell, it was essential that this versioning system happened entirely behind the scenes and did not dictate any specific way in which the prompt engineer needs to facilitate their own process. Therefore, to enable automatic versioning, one simply passes in a storage parameter to the initialization function of ell, where various settings are configured:

.. code-block:: python

    ell.init(store='./logdir')

The argument ``store`` points to either a local path to store data or an ``ell.storage.Store`` object. An ell store is an interface for storing prompts and their invocations, i.e., the input and outputs of a language model program as well as the language model called, generated, and any other metadata. By default, when a path is specified, ell uses a local SQLite DB and an expandable file-based blob store for larger language model programs or invocations that cannot effectively fit into rows of the database.

.. note::
    For production use, ell can utilize a store in any arbitrary database. In the near future, ell will be launching a service similar to Weights & Biases (wandb), where your team can store all prompts in a centralized prompt version control system. This will provide collaborative features and advanced versioning capabilities, much like what wandb offers for machine learning experiments.

When ell is initialized with a store of any kind, anytime a language model program is invoked (actually, the first time it's invoked), the lexical closure of source of that language model program is computed and hashed to create a version hash for that language model program. In addition, the aforementioned dependency graph is computed, and this language model program is then written to the store. After the invocation occurs, all of the input and output data associated with that version of the language model program is also stored in the database for later analysis. As the prompt engineering process continues, new versions of the language model programs are only added to the store if they are invoked at least once.

.. code-block:: python

    import ell
    from ell.stores.sql import SQLiteStore

    ell.init(store='./logdir', autocommit=True)

    @ell.simple(model="gpt-4o-mini")
    def greet(name: str):
        """You are a friendly greeter."""
        return f"Generate a greeting for {name}."

    result = greet("Alice")
    print(result)  # Output: "Hello, Alice! It's wonderful to meet you."

After this execution, a row might be added to the `SerializedLMP` table:

.. code-block:: text

    lmp_id: "1a2b3c4d5e6f7g8h"
    name: "greet"
    source: "@ell.simple(model=\"gpt-4o-mini\")\ndef greet(name: str):\n    \"\"\"You are a friendly greeter.\"\"\"\n    return f\"Generate a greeting for {name}.\""
    dependencies: ""
    created_at: "2023-07-15T10:30:00Z"
    lmp_type: "LM"
    api_params: {"model": "gpt-4o-mini"}
    initial_free_vars: {}
    initial_global_vars: {}
    num_invocations: 1
    commit_message: "Initial version of greet function"
    version_number: 1

And a corresponding row in the `Invocation` table:

.. code-block:: text

    id: "9i8u7y6t5r4e3w2q"
    lmp_id: "1a2b3c4d5e6f7g8h"
    latency_ms: 250.5
    prompt_tokens: 15
    completion_tokens: 10
    created_at: "2023-07-15T10:30:01Z"

With its associated `InvocationContents`:

.. code-block:: text

    invocation_id: "9i8u7y6t5r4e3w2q"
    params: {"name": "Alice"}
    results: ["Hello, Alice! It's wonderful to meet you."]
    invocation_api_params: {"temperature": 1.0, "max_tokens": 50}

This structure allows for efficient tracking and analysis of LMP usage and performance over time.

Autocommitting
~~~~~~~~~~~~~~

Because prompts are just their source code and versions and diffs between versions are automatically computed in the background, we can additionally automatically create human-readable commit messages between versions:

.. code-block:: python

    ell.init(store='./logdir', autocommit=True)

By providing the autocommit=True argument to the initialization function for ell, every time a version is created that supersedes a previous version of a prompt (as collocated by their fully qualified name), ell will use GPT-4-mini to automatically generate a human-readable commit message that can then be viewed later to show effective changes across versions. This works both for the local automatic prompt versioning during prompt engineering to quickly locate an ideal prompt or previous prompt that was developed, and for archival prompt versioning in production when seeking out regressions or previously differently performing language model programs.

.. image:: ../_static/auto_commit.png
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%

Tracing
-------

Central to the prompt engineering process is understanding not just how prompts change, but how they are used.

Traditionally, without a dedicated prompt engineering framework, developers resort to manually storing inputs and outputs from language model API providers. This approach typically involves intercepting API calls and constructing custom database schemas for production applications. However, this method often proves cumbersome, lacking scalability across projects and necessitating frequent re-implementation.

To address these challenges, solutions like Weave and LangChain/LangSmith have emerged, each offering distinct approaches:

1. Function-level tracing: This method captures inputs and outputs of arbitrary Python functions. While effective for monitoring production deployments, it falls short in tracking intra-version changes that often occur during local development and prompt engineering iterations.

2. Framework-specific versioning: This approach, exemplified by LangChain, requires prompts to be versioned within a specific framework. Prompts are typically compressed into template strings or combinations of template strings and versioned Python code. While structured, this method can be restrictive and may not suit all development workflows.

ell takes the best of both worlds by serializing arbitrary Python code. This allows us to track how language model programs are used through their inputs and outputs, organizing these uses by version for later comparison. Importantly, this is achieved without requiring users to do anything more than write normal Python code to produce their prompt strings for the language model API.

Constructing a computation graph
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When using the ell store, all inputs and outputs of language model programs are stored. But what about interactions between them?

To track how language model programs interact during execution and construct a computation graph of data flow (similar to deep learning frameworks like PyTorch and TensorFlow), ell wraps the outputs of all language model programs with a tracing object.

Tracing objects are wrappers around immutable base types in Python. They keep track of originating language model programs and other metadata, preserving this trace of origination across arbitrary operations. One of the most important tracing objects is the _lstr object.

For example, consider the following language model program:

.. code-block:: python

    import ell

    @ell.simple(model="gpt-4o") # version: ae8f32s664200e1
    def hi():
        return "say hi"

    x = hi() # invocation id: 4hdfjhe8ehf (version: ae8f32s664200e1)

While x in this example is functionally a string and behaves exactly like one, it is actually an _lstr:

.. code-block:: python

    >>> type(x)
    <class 'ell.types._lstr.lstr'>

    >>> x
    'hi'

    >>> x.__origin_trace__ 
    {'4hdfjhe8ehf'}

Furthermore, continued manipulation of the string preserves its origin trace, as all original string operations are overridden to produce new immutable instances that contain or combine origin traces.

.. code-block:: python

    >>> x[0]
    'h'

    >>> x[0].__origin_trace__
    {'4hdfjhe8ehf'}

    >>> x + " there"
    'hi there'

    >>> (x + " there").__origin_trace__
    {'4hdfjhe8ehf'}

Additionally, when two mutable objects are combined, the resulting trace is the union of the two traces.

.. code-block:: python

    >>> x = hi() # invocation id: 4hdfjhe8ehf
    >>> y = hi() # invocation id: 345hef345h
    >>> z = x + y
    >>> z.__origin_trace__
    {'4hdfjhe8ehf', '345hef345h'}

By tracking both inputs and outputs of language model programs, we can use these origin traces to construct a computation graph. This graph illustrates how language model programs interact during execution.

This capability allows you to easily track the flow of language model outputs, identify weak points in prompt chains, understand unintended mutations in inputs and outputs of prompts as they are executed, and more generally, create a path for future symbolic and discrete optimization techniques applied to language model programs.

.. image:: ../_static/invocations.webp
   :alt: ell demonstration
   :class: rounded-image invertible-image
   :width: 100%

.. code-block:: python

    @ell.simple(model="gpt-4o-2024-08-06", temperature=1.0)
    def create_personality() -> str:
        """You are backstoryGPT. You come up with a backstory for a character incljuding name. Choose a completely random name from the list. Format as follows.

    Name: <name>
    Backstory: <3 sentence backstory>'""" # System prompt

        return "Come up with a backstory about " + random.choice(names_list) # User prompt


    def format_message_history(message_history : List[Tuple[str, str]]) -> str:
        return "\n".join([f"{name}: {message}" for name, message in message_history])

    @ell.simple(model="gpt-4o-2024-08-06", temperature=0.3, max_tokens=20)
    def chat(message_history : List[Tuple[str, str]], *, personality : str):

            return [
                ell.system(f"""Here is your description.
    {personality}. 

    Your goal is to come up with a response to a chat. Only respond in one sentence (should be like a text message in informality.) Never use Emojis."""),
                ell.user(format_message_history(message_history)),
            ]


.. note::
   Currently, origin tracing in ell works only on string primitives. We're actively developing support for arbitrary object tracking, which will be available in a future release. This enhancement will allow for more comprehensive tracing of various data types throughout your language model programs.

------------------------------------------------


In the next chapter, we will explore how to visualize versioning and tracing data using ell studio. This powerful tool provides a comprehensive interface for analyzing and understanding the complex interactions within your language model programs.============
@ell.complex
============

While ``@ell.simple`` provides a straightforward way to work with language models that return text, modern language models are increasingly capable of handling and generating multimodal content, structured outputs, and complex interactions. This is where ``@ell.complex`` comes into play.

The ``@ell.complex`` decorator is designed to handle sophisticated interactions with language models, including multimodal inputs/outputs, structured data, and tool usage. It extends ``@ell.simple``'s capabilities to address the evolving nature of language models, which can now process images, generate structured data, make function calls, and engage in multi-turn conversations. By returning rich ``Message`` objects instead of simple strings, ``@ell.complex`` enables more nuanced and powerful interactions, overcoming the limitations of traditional string-based interfaces in these advanced scenarios.


.. note:: Messages in ell are not the same as the dictionary messages used in the OpenAI API. ell's Message API provides a more intuitive and flexible way to construct and manipulate messages. You can read more about ell's Message API and type coercion in the :doc:`message_api` page.


Usage
-----

The basic usage of ``@ell.complex`` is similar to ``@ell.simple``, but with enhanced capabilities:

.. code-block:: python

    import ell
    from pydantic import BaseModel, Field

    class MovieReview(BaseModel):
        title: str = Field(description="The title of the movie")
        rating: int = Field(description="The rating of the movie out of 10")
        summary: str = Field(description="A brief summary of the movie")

    @ell.complex(model="gpt-4o-2024-08-06", response_format=MovieReview)
    def generate_movie_review(movie: str):
        """You are a movie review generator. Given the name of a movie, you need to return a structured review."""
        return f"Generate a review for the movie {movie}"

    review_message = generate_movie_review("The Matrix")
    review = review_message.parsed
    print(f"Movie: {review.title}, Rating: {review.rating}/10")
    print(f"Summary: {review.summary}")

Key Features
------------

1. Structured Outputs
^^^^^^^^^^^^^^^^^^^^^

``@ell.complex`` allows for structured outputs using Pydantic models:

.. code-block:: python

    @ell.complex(model="gpt-4o-2024-08-06", response_format=MovieReview)
    def generate_movie_review(movie: str) -> MovieReview:
        """You are a movie review generator. Given the name of a movie, you need to return a structured review."""
        return f"Generate a review for the movie {movie}"

    review_message = generate_movie_review("Inception")
    review = review_message.parsed
    print(f"Rating: {review.rating}/10")

2. Multimodal Interactions
^^^^^^^^^^^^^^^^^^^^^^^^^^

``@ell.complex`` can handle various types of inputs and **outputs**, including text and images:

.. code-block:: python

    from PIL import Image

    @ell.complex(model="gpt-5-omni")
    def describe_and_generate(prompt: str):
        return [
            ell.system("You can describe images and generate new ones based on text prompts."),
            ell.user(prompt)
        ]

    result = describe_and_generate("A serene lake at sunset")
    print(result.text)  # Prints the description
    if result.images:
        result.images[0].show()  # Displays the generated image

3. Chat-based Use Cases
^^^^^^^^^^^^^^^^^^^^^^^

``@ell.complex`` is particularly useful for chat-based applications where you need to maintain conversation history:

.. code-block:: python

    from ell import Message

    @ell.complex(model="gpt-4o", temperature=0.7)
    def chat_bot(message_history: List[Message]) -> List[Message]:
        return [
            ell.system("You are a friendly chatbot. Engage in casual conversation."),
        ] + message_history

    message_history = []
    while True:
        user_input = input("You: ")
        message_history.append(ell.user(user_input))
        response = chat_bot(message_history)
        print("Bot:", response.text)
        message_history.append(response)

4. Tool Usage
^^^^^^^^^^^^^

``@ell.complex`` supports tool usage, allowing language models to make function calls:

.. code-block:: python

    @ell.tool()
    def get_weather(location: str = Field(description="The full name of a city and country, e.g. San Francisco, CA, USA")):
        """Get the current weather for a given location."""
        # Simulated weather API call
        return f"The weather in {location} is sunny."

    @ell.complex(model="gpt-4-turbo", tools=[get_weather])
    def travel_planner(destination: str):
        """Plan a trip based on the destination and current weather."""
        return [
            ell.system("You are a travel planner. Use the weather tool to provide relevant advice."),
            ell.user(f"Plan a trip to {destination}")
        ]

    result = travel_planner("Paris")
    print(result.text)  # Prints travel advice
    if result.tool_calls:
        # This is done so that we can pass the tool calls to the language model
        result_message = result.call_tools_and_collect_as_message()
        print("Weather info:", result_message.tool_results[0].text) # Raw text of the tool call.
        print("Message to be sent to the LLM:", result_message.text) # Representation of the message to be sent to the LLM.


Reference
---------

.. autofunction:: ell.complex
=========== 
Tool Usage
===========


.. warning::
   Tool usage in ell is currently a beta feature and is highly underdeveloped. The API is likely to change significantly in future versions. Use with caution in production environments.

Tool usage is a powerful feature in ell that allows language models to interact with external functions and services. This capability enables the creation of more dynamic and interactive language model programs (LMPs) that can perform actions, retrieve information, and make decisions based on real-time data.

Defining Tools
--------------

In ell, tools are defined using the ``@ell.tool()`` decorator. This decorator transforms a regular Python function into a tool that can be used by language models. Here's an example of a simple tool definition:

.. code-block:: python

    @ell.tool()
    def create_claim_draft(claim_details: str,
                           claim_type: str,
                           claim_amount: float,
                           claim_date: str = Field(description="The date of the claim in the format YYYY-MM-DD.")):
        """Create a claim draft. Returns the claim id created.""" # Tool description
        print("Create claim draft", claim_details, claim_type, claim_amount, claim_date)
        return "claim_id-123234"

The ``@ell.tool()`` decorator automatically generates a schema for the tool based on the function's signature, type annotations, and docstring. This schema is used to provide structured information about the tool to the language model.

Schema Generation
-----------------

ell uses a combination of function inspection and Pydantic models to generate the tool schema. The process involves:

- Extracting parameter information from the function signature.
- Using type annotations to determine parameter types.
- Utilizing Pydantic's ``Field`` for additional parameter metadata.
- Creating a Pydantic model to represent the tool's parameters.

This generated schema is then converted into a format compatible with the OpenAI API. For example:

.. code-block:: python

    {
        "type": "function",
        "function": {
            "name": "create_claim_draft",
            "description": "Create a claim draft. Returns the claim id created.",
            "parameters": {
                "type": "object",
                "properties": {
                    "claim_details": {"type": "string"},
                    "claim_type": {"type": "string"},
                    "claim_amount": {"type": "number"},
                    "claim_date": {
                        "type": "string",
                        "description": "The date of the claim in the format YYYY-MM-DD."
                    }
                },
                "required": ["claim_details", "claim_type", "claim_amount", "claim_date"]
            }
        }
    }

Using Tools in LMPs
-------------------

To use tools in a language model program, you need to specify them in the ``@ell.complex`` decorator:

.. code-block:: python

    @ell.complex(model="gpt-4o", tools=[create_claim_draft], temperature=0.1)
    def insurance_claim_chatbot(message_history: List[Message]) -> List[Message]:
        return [
            ell.system("""You are an insurance adjuster AI. You are given a dialogue with a user and have access to various tools to effectuate the insurance claim adjustment process. Ask questions until you have enough information to create a claim draft. Then ask for approval."""),
        ] + message_history

This allows the language model to access and use the specified tools within the context of the LMP.

Single-Step Tool Usage
----------------------

In single-step tool usage, the language model decides to use a tool once during its execution. The process typically involves the LMP receiving input, generating a response with a tool call. 

Here's an example where we want to take a natural language string for a website and convert it into a URL to get its content. We'll call this LMP ``get_website_content``, and it will allow the user to get the HTML page of any website they ask for in natural language. The chief goal of the language model here is to convert the website description into a URL and then invoke the ``get_html_content`` tool. The language model also has the option to refuse the request if no such website exists within its knowledge base.

.. code-block:: python

    @ell.tool()
    def get_html_content(
        url: str = Field(description="The URL to get the HTML content of. Never include the protocol (like http:// or https://)"),
    ):
        """Get the HTML content of a URL."""
        response = requests.get("https://" + url)
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text()[:100]

    @ell.complex(model="gpt-4o", tools=[get_html_content])
    def get_website_content(website: str) -> str:
        """You are an agent that can summarize the contents of a website."""
        return f"Tell me what's on {website}"

.. code-block:: python

    >>> output = get_website_content("new york times front page")
    Message(role='assistant', content=[ContenBlock(tool_call=ToolCall(id='tool_call_id', function=Function(name='get_html_content', arguments='{"url": "nyt.com"}'))])

    >>> if output.tool_calls: print(output.tool_calls[0]())
    '''<html lang="en" class="nytapp-vi-homepage nytapp-vi-homepage " xmlns:og="http://opengraphprotocol.org/schema/" data-rh="lang,class"><head>
    <meta charset="utf-8">
    <title>The New York Times - Breaking News, US News, World News and Videos</title>
    <meta'''

We could also handle text based message Responses from the language model where it. may decline to call the tool. the tool or ask for clarification By looking into output text only. In this case, because the language model decided to call the tool, this should be empty. 

.. code-block:: python

    >>> if output.text_only: print(output.text_only)
    None

Multi-Step Tool Usage
---------------------

Multi-step tool usage involves a more complex interaction where the language model may use tools multiple times in a conversation or processing flow. This is particularly useful for chatbots or interactive systems. 

In a typical LLM API the flow for multi-step tool usage looks like this

.. code-block::  

    1. You call the LLM with a message
    2. The LLM returns a message with tool Call
    3. You call the tools on your end and format the results back into a message
    4. You call the LLM with the tool result message
    5. The LLM returns a message with it's final response

This process can be error-prone and requires a lot of boilerplate code. 
To simplify this process, ell provides a helper function ``call_tools_and_collect_as_message()``. This function executes all tool calls in a response and collects the results into a single message, which can then be easily added to the conversation history.

Here's an example of a multi-step interaction using the insurance claim chatbot:

.. code-block:: python

    @ell.complex(model="gpt-4o", tools=[create_claim_draft], temperature=0.1)
    def insurance_claim_chatbot(message_history: List[Message]) -> List[Message]:
        return [
            ell.system("""You are an insurance adjuster AI. You are given a dialogue with a user and have access to various tools to effectuate the insurance claim adjustment process. Ask questions until you have enough information to create a claim draft. Then ask for approval."""),
        ] + message_history

    message_history = []
    user_messages = [
        "Hello, I'm a customer",
        'I broke my car',
        ' smashed by someone else, today, $5k',
        'please file it.'
    ]
    for user_message in user_messages:
        message_history.append(ell.user(user_message))
        response_message = insurance_claim_chatbot(message_history)
        message_history.append(response_message)

        if response_message.tool_calls:
            next_message = response_message.call_tools_and_collect_as_message()
            message_history.append(next_message)
            insurance_claim_chatbot(message_history)



Parallel Tool Execution
~~~~~~~~~~~~~~~~~~~~~~~

For efficiency, ell supports parallel execution of multiple tool calls:

.. code-block:: python

    if response.tool_calls:
        tool_results = response.call_tools_and_collect_as_message(parallel=True, max_workers=3)

This can significantly speed up operations when multiple independent tool calls are made.

Future Features: Eager Mode
---------------------------

In the future, ell may introduce an "eager mode" for tool usage. This feature would automatically execute tool calls made by the language model, creating a multi-step interaction behind the scenes. This could streamline the development process by reducing the need for explicit tool call handling in the code.

Eager mode could potentially work like this:

- The LMP generates a response with a tool call.
- ell automatically executes the tool and captures its result.
- The result is immediately fed back into the LMP for further processing.
- This cycle continues until the LMP generates a final response without tool calls.

This feature would make it easier to create complex, multi-step interactions without the need for explicit loop handling in the user code. It would be particularly useful for scenarios where the number of tool calls is not known in advance, such as in open-ended conversations or complex problem-solving tasks.

Future Features: Tool Spec Autogeneration
-------------------------------------------

.. note:: Thanks to `Aidan McLau <https://x.com/aidan_mclau>`_ for suggesting this feature.

In an ideal world, a prompt engineering library would not require the user to meticulously specify the schema for a tool. Instead, a language model should be able to infer the tool specification directly from the source code of the tool. In ell, we can extract the lexically closed source of any Python function, enabling a feature where the schema is automatically generated by another language model when a tool is given to an ell decorator.

This approach eliminates the need for users to manually type every argument and provide a tool description, as the description becomes implicit from the source code.

Consider the following example function in a user's code:

.. code-block:: python

   def search_twitter(query, n=7):
    # Query must be three words or less
    async def fetch_tweets():
        api = API()
        await api.pool.login_all()
        try:
            tweets = [tweet async for tweet in api.search(query, limit=n)]
    
            tweet_strings = [
                f"Search Query: {query}\n"
                f"Author: {tweet.user.username}\n"
                f"Tweet: {tweet.rawContent}\n"
                f"Created at: {tweet.date}\n"
                f"Favorites: {tweet.likeCount}\n"
                f"Retweets: {tweet.retweetCount}\n\n\n" for tweet in tweets
            ]
            if tweet_strings:
                print(tweet_strings[0])  # Print the first tweet
            return tweet_strings
        except Exception as e:
            print(f"Error fetching search tweets: {e}")
            return []
    
    tweets = asyncio.run(fetch_tweets())
    tweets = tweets[:n]
    tweets = "<twitter_results>" + "\n".join(tweets) + "</twitter_results>"
    return tweets


With tool spec autogeneration, the user could wrap this search_twitter function with a tool decorator and an optional parameter to automatically generate the tool spec. The following specification would be generated:

.. code-block:: python

    @ell.tool(autogenerate=True)
    def search_twitter(query, n=7):
        ...


.. code-block:: json
    :emphasize-lines: 5, 5, 11, 11

    {
      "type": "function",
      "function": {
        "name": "search_twitter",
        "description": "Search Twitter for tweets",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {
              "type": "string",
              "description": "The query to search for, this must be three words or less"
            },
            "n": {
              "type": "integer",
              "description": "The number of tweets to return"
            }
          },
          "required": ["query"]
        }
      }
    }

This is accomplished by a language model program that takes the source code of a tool function as input and generates the corresponding tool specification.


.. code-block:: python

    @ell.simple(model="claude-3-5-sonnet", temperature=0.0)
    def generate_tool_spec(tool_source: str):
        '''
        You are a helpful assistant that takes in source code for a python function and produces a JSON schema for the function.

        Here is an example schema
        {
            "type": "function",
            "function": {
                "name": "some_tool",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "some_arg": {
                            "type": "string",
                            "description": "This is a description of the argument"
                        }
                    },
                    "required": ["some_arg"]
                }
            }
        }
        '''

        return f"Generate a tool spec for the following function: {tool_source}"

    # ...
    # When autogenerate is called.
    auto_tool_spec = json.loads(generate_tool_spec(search_twitter))

For this approach to be effective, the generate tool spec calls should only be executed when the version of the tool source code changes. Refer to the versioning and tracing section for details on how this is computed. Additionally, the generated tool spec would need to be stored in a consistent and reusable ell store.

This approach does present some potential challenges:

1. It introduces opinions into ell as a library by including a specific prompt for automatically generating tool specs.
2. It may compromise consistency regarding the reproducibility of prompts.

To address the issue of opinionation, we could require users to implement their own prompt for automatically generating tool specs from source code. While ell could offer some pre-packaged options, it would require users to make a conscious decision to use this auto-generation function, as it has more significant consequences than, for example, auto-committing.

.. code-block:: python

    @ell.simple
    def my_custom_tool_spec_generator(tool_source: str):
        # User implements this once in their code base or repo
        ...

    @ell.tool(autogenerate=my_custom_tool_spec_generator)
    def search_twitter(query, n=7):
        ...

    @ell.complex(model="gpt-4o", tools=[search_twitter])
    def my_llm_program(message_history: List[Message]) -> List[Message]:
        ...


The reproducibility aspect can be mitigated by serializing the generated tool specification along with its version in the ell store. This ensures that all invocations depend on the specific generated tool specification, maintaining consistency across different runs.

.. code-block:: python

    >>> lexical_closure(search_twitter)
    """
    @ell.simple
    def my_custom_tool_spec_generator(tool_source: str):
        # User implements this
        ...
    
    _generated_spec = my_custom_tool_spec_generator(lexical_closure(search_twitter))
    '''
      {
      "type": "function",
      "function": {
        "name": "search_twitter",
        "description": "Search Twitter for tweets",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {
              "type": "string",
              "description": "The query to search for, this must be three words or less"
            },
            "n": {
              "type": "integer",
              "description": "The number of tweets to return"
            }
          },
          "required": ["query"]
        }
      }
    }
    '''

    @ell.tool(toolspec=_generated_spec)
    def search_twitter(query, n=7):
        ...

Furthermore, consistency can be enforced by requiring specification generators to use a temperature of 0.0 and be near-deterministic in their output. This approach ensures that the generated tool specifications remain consistent across different runs, enhancing reproducibility and reliability in the tool generation process.
